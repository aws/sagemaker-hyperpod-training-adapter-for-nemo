trainer:
  devices: 8
  num_nodes: 1
  accelerator: gpu
  precision: bf16
  max_steps: 50
  log_every_n_steps: 1
  val_check_interval: 0
  limit_val_batches: 0 # Number of batches per each validation run, set to 0 to disable validation.


exp_manager:
  exp_dir: ???
  name: experiment
  create_tensorboard_logger: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    # Set save_top_k = 0 to disable sharded checkpointing
    save_top_k: 0
    every_n_train_steps: 10
    monitor: "step"
    mode: "max"
    save_last: True
  checkpoint_dir: ${exp_manager.exp_dir}/checkpoints/
  resume_from_checkpoint: null
  # Set auto_checkpoint = False and save_last = False to disable auto resilience checkpointing
  auto_checkpoint:
    enabled: False
  export_full_model:
    # Set every_n_train_steps = 0 and save_last = False to disable full checkpointing
    every_n_train_steps: 0
    save_last: False

use_smp_model: False #disabable SMP
distributed_backend: nccl


# Start training from pretrained model
model:
  model_type: llama_v3 # TODO: need functionality to parse model type automatically, this arg can stil be kept
  train_batch_size: 2
  context_parallel_degree: 1
  moe: False
  activation_checkpointing: True
  activation_loading_horizon: 2
  delayed_param: False
  offload_activations: False
  seed: 12345
  grad_clip: 1.0


  # FSDP Configs
  sharding_strategy: hybrid_shard
  forward_prefetch: True
  shard_degree: 8
  backward_fetch_policy: backward_pre
  auto_wrap_policy: transformer_auto_wrap_policy
  limit_all_gathers: False
  use_orig_param: False

  # model architecture
  max_context_width: 8192
  max_position_embeddings: ${.max_context_width}
  num_hidden_layers: 32
  hidden_size: 4096
  num_heads: 32
  intermediate_size: 14336
  initializer_range: 0.02
  layernorm_epsilon: 1e-5
  vocab_size: 128256
  num_key_value_heads: null
  use_flash_attention: True
  rope_theta: 10000.0

  # rope scaling
  rope_scaling:
    rope_type: llama3
    factor: 8.0
    high_freq_factor: 4.0
    low_freq_factor: 1.0
    original_max_position_embeddings: 8192

  # Transformer Engine
  fp8: False
  fp8_amax_history_len: 1024
  fp8_amax_compute_algo: max

  do_finetune: True
  hf_model_name_or_path: "/fsx/hf_pretrained_models/Meta-Llama-3-8B"

  precision: ${trainer.precision}

  lr_decay_iters: ${trainer.max_steps}

  log_reduced_training_loss: True

  # Optimizer
  optim:
    name: adamw
    lr: 2e-4
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 0
      constant_steps: 0
      min_lr: 2e-5

  # Data
  data:
    train_dir: ["/fsx/datasets/c4/en/hf-tokenized/llama3/train"]
    val_dir: ["/fsx/datasets/c4/en/hf-tokenized/llama3/val"]
    dataset_type: hf
    use_synthetic_data: True

  # Viztracer
  viztracer:
    enabled: False

  # PEFT
  peft:
    peft_type: lora
    rank: 32
    alpha: 16
    dropout: 0.1
